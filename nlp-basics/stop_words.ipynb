{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "__What are Stop Words?__\n",
    "Stop Words are words that are commonly occuring and may contain little informative value when it comes to NLP tasks such as topic modeling or document classification. \n",
    "\n",
    "__Why do we care?__\n",
    "Because Stop Words can appear to contribute very little to the understanding of a document, their removal can simplify things for us, such as reducing sparsity of matrix representations of textual data.  However, what was once common practice has since become less standard, especially when one wants to use deep learning methods.  In some cases, we may want to forego Stop Word Removal altogether!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Getting a list of Stop Words__ \n",
    "We can get a list of English Stop Words from libraries like SciKit Learn & SpaCy.  Please note that at some point in the future SciKit Learn may once again change how these methods are imported and employed, so check the documentation if errors or deprecation warnings are reported.   As of January 2021, the following should be sufficient for SciKit Learn.\n",
    "\n",
    "__Be Aware:__ \n",
    "There is no universal list of Stop Words!  What SciKit Learn considers to be Stop Words won't match exactly with what another library, like SpaCy, considers to be Stop Words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Stop Words from SciKit Learn\n",
    "Note that we don't always have to get the Stop Word lists from the CountVectorizer submodule.  It exists as a method attached to every text submodule used to build feature vectors from text documents.  So for example, we could've replaced CountVectorizer with TfidVectorizer instead and still retrieved the list of English Stop Words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(stop_words = \"english\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 20 Stop Words in SciKit Learn's List of English Stop Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(cv.get_stop_words()))[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Stop Words from SpaCy \n",
    "NOTE (SpaCy vs NLTK):  SpaCy is opinionated.  When you want to do something in SpaCy, it makes you do it \"the SpaCy way\", which tends to be very well optimized.  NLTK, in contrast, gives you a lot more variety in terms of getting the same thing done.  \n",
    "\n",
    "SpaCy can be set to use different 'statisical models' that contain info regarding the language you want to use.  In this case, we're using an English model, that's of Type Core (general-purpose) that was trained on Web data, and is of a small size (occupies less memory on disk).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(nlp.Defaults.stop_words))[6:26] # Note that we had to set indices from 6 to 26 to get a comparable list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing SciKit Learn Stop Words to SpaCy Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_stop_words = sorted(list(cv.get_stop_words()))\n",
    "spacy_stop_words = sorted(list(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Get the common stop words between SciKit Learn and SpaCy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amount']\n"
     ]
    }
   ],
   "source": [
    "common_stop_words = [] \n",
    "for i in sklearn_stop_words:\n",
    "    for j in spacy_stop_words:\n",
    "        if i == j:\n",
    "            common_stop_words.append(i)\n",
    "print(common_stop_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What stop words are in sklearn but not spacy?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amoungst', 'bill', 'cant', 'co', 'con', 'couldnt', 'cry', 'de', 'describe', 'detail', 'eg', 'etc', 'fill', 'find', 'fire', 'found', 'hasnt', 'ie', 'inc', 'interest', 'ltd', 'mill', 'sincere', 'system', 'thick', 'thin', 'un']\n"
     ]
    }
   ],
   "source": [
    "sklearn_not_spacy = []\n",
    "for i in sklearn_stop_words:\n",
    "    if i not in spacy_stop_words:\n",
    "        sklearn_not_spacy.append(i)\n",
    "print(sklearn_not_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What stop words are in spacy but not sklearn?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'ca', 'did', 'does', 'doing', 'just', 'make', \"n't\", 'n‘t', 'n’t', 'quite', 'really', 'regarding', 'say', 'unless', 'used', 'using', 'various', '‘d', '‘ll', '‘m', '‘re', '‘s', '‘ve', '’d', '’ll', '’m', '’re', '’s', '’ve']\n"
     ]
    }
   ],
   "source": [
    "spacy_not_sklearn = [] \n",
    "for i in spacy_stop_words:\n",
    "    if i not in sklearn_stop_words:\n",
    "        spacy_not_sklearn.append(i)\n",
    "print(spacy_not_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words in One Line with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      ", Dave, watched, as, the, forest, burned, up, on, the]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Dave watched as the forest burned up on the hill,\n",
    "only a few miles from his house. The car had\n",
    "been hastily packed and Marta was inside trying to round\n",
    "up the last of the pets. \"Where could she be?\" he wondered\n",
    "as he continued to wait for Marta to appear with the pets.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "# Tokenize and put into a list in one line\n",
    "token_list = [token for token in doc]\n",
    "print(token_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      ", Dave, watched, forest, burned, hill, ,, \n",
      ", miles, house]\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words in one line using hte .is_stop attribute\n",
    "filtered_tokens = [token for token in doc if not token.is_stop]\n",
    "print(filtered_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
